{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873a05f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score, adjusted_rand_score\n",
    "import json\n",
    "import os\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8f4407",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "for i in range(1, 4):\n",
    "    df = pd.read_csv(f'data/S07-hw-dataset-0{i}.csv')\n",
    "    print(f'\\nDataset {i}:')\n",
    "    print(f'  Shape: {df.shape}')\n",
    "    print(f'  Head:\\n{df.head()}')\n",
    "    print(f'  Info:')\n",
    "    df.info()\n",
    "    print(f'  Describe:\\n{df.describe()}')\n",
    "    print(f'  Missing values:\\n{df.isnull().sum()}')\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    if 'sample_id' in df.columns:\n",
    "        numeric_cols.remove('sample_id') if 'sample_id' in numeric_cols else None\n",
    "    \n",
    "    data[f'ds{i}'] = {'df': df, 'numeric_cols': numeric_cols}\n",
    "    print(f'  Numeric cols: {numeric_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef024aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = {}\n",
    "\n",
    "for ds_name, ds_data in data.items():\n",
    "    df = ds_data['df']\n",
    "    numeric_cols = ds_data['numeric_cols']\n",
    "    \n",
    "    sample_id = df['sample_id'] if 'sample_id' in df.columns else df.index\n",
    "    X = df[numeric_cols].copy()\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    X_processed = pd.DataFrame(\n",
    "        pipeline.fit_transform(X),\n",
    "        columns=numeric_cols\n",
    "    )\n",
    "    \n",
    "    preprocessed[ds_name] = {\n",
    "        'X': X_processed,\n",
    "        'sample_id': sample_id,\n",
    "        'pipeline': pipeline\n",
    "    }\n",
    "    \n",
    "    print(f'{ds_name}: processed {X_processed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff6c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_results = {}\n",
    "\n",
    "for ds_name, prep_data in preprocessed.items():\n",
    "    X = prep_data['X']\n",
    "    print(f'\\n{ds_name} - KMeans:')\n",
    "    \n",
    "    k_range = range(2, 21)\n",
    "    sil_scores = []\n",
    "    db_scores = []\n",
    "    models = []\n",
    "    \n",
    "    for k in k_range:\n",
    "        km = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = km.fit_predict(X)\n",
    "        sil = silhouette_score(X, labels)\n",
    "        db = davies_bouldin_score(X, labels)\n",
    "        \n",
    "        sil_scores.append(sil)\n",
    "        db_scores.append(db)\n",
    "        models.append(km)\n",
    "        print(f'  k={k:2d}: sil={sil:.4f}, db={db:.4f}')\n",
    "    \n",
    "    best_k_idx = np.argmax(sil_scores)\n",
    "    best_k = list(k_range)[best_k_idx]\n",
    "    \n",
    "    kmeans_results[ds_name] = {\n",
    "        'best_k': best_k,\n",
    "        'best_model': models[best_k_idx],\n",
    "        'sil_scores': sil_scores,\n",
    "        'db_scores': db_scores,\n",
    "        'k_range': list(k_range),\n",
    "        'models': models\n",
    "    }\n",
    "    \n",
    "    print(f'  Best k: {best_k}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ce5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "fig.suptitle('KMeans: Silhouette vs k', fontsize=14, fontweight='bold')\n",
    "\n",
    "for idx, (ds_name, res) in enumerate(kmeans_results.items()):\n",
    "    ax = axes[idx]\n",
    "    k_range = res['k_range']\n",
    "    sil_scores = res['sil_scores']\n",
    "    best_k = res['best_k']\n",
    "    \n",
    "    ax.plot(k_range, sil_scores, 'b-o', linewidth=2, markersize=6)\n",
    "    ax.axvline(best_k, color='r', linestyle='--', linewidth=2, label=f'best k={best_k}')\n",
    "    ax.scatter([best_k], [sil_scores[best_k-2]], color='r', s=200, zorder=5)\n",
    "    \n",
    "    ax.set_xlabel('k')\n",
    "    ax.set_ylabel('Silhouette Score')\n",
    "    ax.set_title(ds_name)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "os.makedirs('artifacts/figures', exist_ok=True)\n",
    "plt.savefig('artifacts/figures/kmeans_silhouette_analysis.png', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print('сохранено: kmeans_silhouette_analysis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf9c1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan_results = {}\n",
    "\n",
    "for ds_name, prep_data in preprocessed.items():\n",
    "    X = prep_data['X']\n",
    "    print(f'\\n{ds_name} - DBSCAN:')\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    eps_range = np.linspace(0.2, 2.0, 10)\n",
    "    min_samples_values = [3, 5, 10]\n",
    "    \n",
    "    best_sil = -1\n",
    "    best_params = {}\n",
    "    best_model = None\n",
    "    all_results = []\n",
    "    \n",
    "    for min_samples in min_samples_values:\n",
    "        for eps in eps_range:\n",
    "            db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "            labels = db.fit_predict(X)\n",
    "            \n",
    "            n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "            n_noise = list(labels).count(-1)\n",
    "            \n",
    "            sil = -1\n",
    "            if n_clusters > 0 and n_noise < len(X):\n",
    "                try:\n",
    "                    sil = silhouette_score(X, labels, sample_size=min(500, X.shape[0]))\n",
    "                except:\n",
    "                    sil = -1\n",
    "            \n",
    "            all_results.append({\n",
    "                'eps': eps,\n",
    "                'min_samples': min_samples,\n",
    "                'n_clusters': n_clusters,\n",
    "                'n_noise': n_noise,\n",
    "                'sil': sil,\n",
    "                'model': db\n",
    "            })\n",
    "            \n",
    "            if sil > best_sil and n_clusters > 0:\n",
    "                best_sil = sil\n",
    "                best_params = {'eps': eps, 'min_samples': min_samples}\n",
    "                best_model = db\n",
    "    \n",
    "    valid_results = [r for r in all_results if r['sil'] > 0]\n",
    "    if valid_results:\n",
    "        best_result = max(valid_results, key=lambda x: x['sil'])\n",
    "    else:\n",
    "        best_result = max(all_results, key=lambda x: x['n_clusters'])\n",
    "    \n",
    "    dbscan_results[ds_name] = {\n",
    "        'best_params': {'eps': best_result['eps'], 'min_samples': best_result['min_samples']},\n",
    "        'best_model': best_result['model'],\n",
    "        'n_clusters': best_result['n_clusters'],\n",
    "        'n_noise': best_result['n_noise'],\n",
    "        'sil': best_result['sil'],\n",
    "        'all_results': all_results\n",
    "    }\n",
    "    \n",
    "    print(f'  Лучшие параметры: eps={best_result[\"eps\"]:.2f}, min_samples={best_result[\"min_samples\"]}')\n",
    "    print(f'  Кластеры: {best_result[\"n_clusters\"]}, Шум: {best_result[\"n_noise\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a77b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "\n",
    "for ds_name, prep_data in preprocessed.items():\n",
    "    X = prep_data['X']\n",
    "    print(f'\\n{ds_name} - Metrics:')\n",
    "    \n",
    "    km_model = kmeans_results[ds_name]['best_model']\n",
    "    km_labels = km_model.fit_predict(X)\n",
    "    \n",
    "    km_sil = silhouette_score(X, km_labels)\n",
    "    km_db = davies_bouldin_score(X, km_labels)\n",
    "    km_ch = calinski_harabasz_score(X, km_labels)\n",
    "    \n",
    "    metrics[ds_name] = {\n",
    "        'KMeans': {\n",
    "            'silhouette': km_sil,\n",
    "            'davies_bouldin': km_db,\n",
    "            'calinski_harabasz': km_ch,\n",
    "            'n_clusters': kmeans_results[ds_name]['best_k']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(f'  KMeans: sil={km_sil:.4f}, db={km_db:.4f}, ch={km_ch:.1f}')\n",
    "    \n",
    "    db_model = dbscan_results[ds_name]['best_model']\n",
    "    db_labels = db_model.fit_predict(X)\n",
    "    \n",
    "    n_clusters_db = len(set(db_labels)) - (1 if -1 in db_labels else 0)\n",
    "    n_noise = list(db_labels).count(-1)\n",
    "    noise_ratio = n_noise / len(X)\n",
    "    \n",
    "    db_sil = -1\n",
    "    db_db = -1\n",
    "    db_ch = -1\n",
    "    \n",
    "    if n_clusters_db > 0:\n",
    "        try:\n",
    "            db_sil = silhouette_score(X, db_labels, sample_size=min(500, X.shape[0]))\n",
    "            db_db = davies_bouldin_score(X, db_labels)\n",
    "            db_ch = calinski_harabasz_score(X, db_labels)\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    metrics[ds_name]['DBSCAN'] = {\n",
    "        'silhouette': db_sil,\n",
    "        'davies_bouldin': db_db,\n",
    "        'calinski_harabasz': db_ch,\n",
    "        'n_clusters': n_clusters_db,\n",
    "        'noise_points': n_noise,\n",
    "        'noise_ratio': noise_ratio\n",
    "    }\n",
    "    \n",
    "    print(f'  DBSCAN: sil={db_sil:.4f}, db={db_db:.4f}, ch={db_ch:.1f}')\n",
    "    print(f'  Noise: {n_noise} ({noise_ratio*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2ecc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "fig.suptitle('PCA 2D Clustering Visualization', fontsize=14, fontweight='bold')\n",
    "\n",
    "ds_names = list(preprocessed.keys())\n",
    "\n",
    "for row, ds_name in enumerate(ds_names):\n",
    "    X = preprocessed[ds_name]['X']\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    \n",
    "    ax = axes[row, 0]\n",
    "    km_labels = kmeans_results[ds_name]['best_model'].predict(X)\n",
    "    scatter = ax.scatter(X_pca[:, 0], X_pca[:, 1], c=km_labels, cmap='viridis', s=30, alpha=0.6)\n",
    "    ax.set_title(f'{ds_name} - KMeans (k={kmeans_results[ds_name][\"best_k\"]})')\n",
    "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "    plt.colorbar(scatter, ax=ax)\n",
    "    \n",
    "    ax = axes[row, 1]\n",
    "    db_labels = dbscan_results[ds_name]['best_model'].fit_predict(X)\n",
    "    noise_mask = db_labels == -1\n",
    "    \n",
    "    scatter = ax.scatter(X_pca[~noise_mask, 0], X_pca[~noise_mask, 1], \n",
    "                        c=db_labels[~noise_mask], cmap='viridis', s=30, alpha=0.6)\n",
    "    ax.scatter(X_pca[noise_mask, 0], X_pca[noise_mask, 1], \n",
    "              marker='x', c='red', s=50, label='Noise')\n",
    "    \n",
    "    ax.set_title(f'{ds_name} - DBSCAN (n_clusters={dbscan_results[ds_name][\"n_clusters\"]})')\n",
    "    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%})')\n",
    "    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%})')\n",
    "    if noise_mask.sum() > 0:\n",
    "        ax.legend()\n",
    "    plt.colorbar(scatter, ax=ax)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/figures/pca_clustering_2d.png', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print('сохранено: pca_clustering_2d.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff228c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "fig.suptitle('DBSCAN Parameters Analysis', fontsize=14, fontweight='bold')\n",
    "\n",
    "ds_names = list(preprocessed.keys())\n",
    "\n",
    "for row, ds_name in enumerate(ds_names):\n",
    "    all_results = dbscan_results[ds_name]['all_results']\n",
    "    best_eps = dbscan_results[ds_name]['best_params']['eps']\n",
    "    best_min_samples = dbscan_results[ds_name]['best_params']['min_samples']\n",
    "    \n",
    "    eps_vals = sorted(set(r['eps'] for r in all_results))\n",
    "    \n",
    "    ax = axes[row, 0]\n",
    "    for min_samples in [3, 5, 10]:\n",
    "        results = [r for r in all_results if r['min_samples'] == min_samples]\n",
    "        eps_plot = [r['eps'] for r in results]\n",
    "        n_clusters = [r['n_clusters'] for r in results]\n",
    "        ax.plot(eps_plot, n_clusters, 'o-', label=f'min_samples={min_samples}', linewidth=2)\n",
    "    \n",
    "    ax.scatter([best_eps], [dbscan_results[ds_name]['n_clusters']], \n",
    "              s=200, marker='*', c='red', edgecolors='black', linewidth=1.5, zorder=5)\n",
    "    ax.set_xlabel('eps')\n",
    "    ax.set_ylabel('Number of clusters')\n",
    "    ax.set_title(f'{ds_name} - Clusters vs eps')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "    \n",
    "    ax = axes[row, 1]\n",
    "    for min_samples in [3, 5, 10]:\n",
    "        results = [r for r in all_results if r['min_samples'] == min_samples]\n",
    "        eps_plot = [r['eps'] for r in results]\n",
    "        noise_ratio = [(r['n_noise'] / len(preprocessed[ds_name]['X'])) * 100 for r in results]\n",
    "        ax.plot(eps_plot, noise_ratio, 's-', label=f'min_samples={min_samples}', linewidth=2)\n",
    "    \n",
    "    best_noise_ratio = (dbscan_results[ds_name]['n_noise'] / len(preprocessed[ds_name]['X'])) * 100\n",
    "    ax.scatter([best_eps], [best_noise_ratio], \n",
    "              s=200, marker='*', c='red', edgecolors='black', linewidth=1.5, zorder=5)\n",
    "    ax.set_xlabel('eps')\n",
    "    ax.set_ylabel('Noise ratio (%)')\n",
    "    ax.set_title(f'{ds_name} - Noise vs eps')\n",
    "    ax.legend()\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/figures/dbscan_parameters_analysis.png', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print('сохранено: dbscan_parameters_analysis.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f60d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "fig.suptitle('Metrics Comparison: KMeans vs DBSCAN', fontsize=14, fontweight='bold')\n",
    "\n",
    "ds_names = list(preprocessed.keys())\n",
    "x = np.arange(len(ds_names))\n",
    "width = 0.35\n",
    "\n",
    "km_sil = [metrics[d]['KMeans']['silhouette'] for d in ds_names]\n",
    "db_sil = [metrics[d]['DBSCAN']['silhouette'] for d in ds_names]\n",
    "\n",
    "axes[0].bar(x - width/2, km_sil, width, label='KMeans', alpha=0.8)\n",
    "axes[0].bar(x + width/2, db_sil, width, label='DBSCAN', alpha=0.8)\n",
    "axes[0].set_ylabel('Silhouette Score')\n",
    "axes[0].set_title('Silhouette (higher is better)')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(ds_names)\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "km_db = [metrics[d]['KMeans']['davies_bouldin'] for d in ds_names]\n",
    "db_db = [metrics[d]['DBSCAN']['davies_bouldin'] for d in ds_names]\n",
    "\n",
    "axes[1].bar(x - width/2, km_db, width, label='KMeans', alpha=0.8)\n",
    "axes[1].bar(x + width/2, db_db, width, label='DBSCAN', alpha=0.8)\n",
    "axes[1].set_ylabel('Davies-Bouldin Index')\n",
    "axes[1].set_title('Davies-Bouldin (lower is better)')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(ds_names)\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "km_ch = [metrics[d]['KMeans']['calinski_harabasz'] for d in ds_names]\n",
    "db_ch = [metrics[d]['DBSCAN']['calinski_harabasz'] for d in ds_names]\n",
    "\n",
    "axes[2].bar(x - width/2, km_ch, width, label='KMeans', alpha=0.8)\n",
    "axes[2].bar(x + width/2, db_ch, width, label='DBSCAN', alpha=0.8)\n",
    "axes[2].set_ylabel('Calinski-Harabasz Index')\n",
    "axes[2].set_title('Calinski-Harabasz (higher is better)')\n",
    "axes[2].set_xticks(x)\n",
    "axes[2].set_xticklabels(ds_names)\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('artifacts/figures/metrics_comparison.png', dpi=100)\n",
    "plt.show()\n",
    "\n",
    "print('сохранено: metrics_comparison.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1c58d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nПроверка устойчивости (Датасет 1 - KMeans):')\n",
    "\n",
    "X = preprocessed['ds1']['X']\n",
    "n_runs = 5\n",
    "ari_scores = []\n",
    "random_states = [42, 123, 456, 789, 999]\n",
    "\n",
    "labels_all = []\n",
    "for rs in random_states:\n",
    "    km = KMeans(n_clusters=kmeans_results['ds1']['best_k'], random_state=rs, n_init=10)\n",
    "    labels = km.fit_predict(X)\n",
    "    labels_all.append(labels)\n",
    "\n",
    "for i in range(1, len(labels_all)):\n",
    "    ari = adjusted_rand_score(labels_all[0], labels_all[i])\n",
    "    ari_scores.append(ari)\n",
    "    print(f'  ARI(run0, run{i}): {ari:.4f}')\n",
    "\n",
    "print(f'  Средний ARI: {np.mean(ari_scores):.4f}')\n",
    "print(f'  Стд ARI: {np.std(ari_scores):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952099e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nАнализ датасетов и выводы:')\n",
    "\n",
    "print('\\nДатасет 1 (12000 строк, 8 столбец):')\n",
    "print('  Структура: Хорошо разделённые кластеры')\n",
    "print('  Лучший метод: KMeans (k=2)')\n",
    "print('  Обоснование: Высокий силуэтный коэффициент (0.52), стабильная кластеризация.')\n",
    "print('  Проблемы: Нет значительных проблем.')\n",
    "\n",
    "print('\\nДатасет 2 (8000 строк, 3 столбец):')\n",
    "print('  Структура: Плотная, сложная структура')\n",
    "print('  Лучший метод: DBSCAN')\n",
    "print('  Обоснование: Естественно обрабатывает шум (~2%), гибкое обнаружение кластеров.')\n",
    "print('  Проблемы: Чувствительность к параметру eps.')\n",
    "\n",
    "print('\\nДатасет 3 (15000 строк, 4 столбец):')\n",
    "print('  Структура: Большой датасет с шумом (~21.6%)')\n",
    "print('  Лучший метод: KMeans (k=3)')\n",
    "print('  Обоснование: Более стабилен чем DBSCAN при наличии шума.')\n",
    "print('  Проблемы: Высокое соотношение шума влияет на оба алгоритма.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d47d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('artifacts', exist_ok=True)\n",
    "\n",
    "metrics_summary = {}\n",
    "for ds_name in preprocessed.keys():\n",
    "    metrics_summary[ds_name] = {\n",
    "        'size': {'samples': int(preprocessed[ds_name]['X'].shape[0]), \n",
    "                'features': int(preprocessed[ds_name]['X'].shape[1])},\n",
    "        'KMeans': {\n",
    "            'n_clusters': int(kmeans_results[ds_name]['best_k']),\n",
    "            'silhouette': float(metrics[ds_name]['KMeans']['silhouette']),\n",
    "            'davies_bouldin': float(metrics[ds_name]['KMeans']['davies_bouldin']),\n",
    "            'calinski_harabasz': float(metrics[ds_name]['KMeans']['calinski_harabasz'])\n",
    "        },\n",
    "        'DBSCAN': {\n",
    "            'eps': float(dbscan_results[ds_name]['best_params']['eps']),\n",
    "            'min_samples': int(dbscan_results[ds_name]['best_params']['min_samples']),\n",
    "            'n_clusters': int(metrics[ds_name]['DBSCAN']['n_clusters']),\n",
    "            'n_noise': int(metrics[ds_name]['DBSCAN']['noise_points']),\n",
    "            'noise_ratio': float(metrics[ds_name]['DBSCAN']['noise_ratio']),\n",
    "            'silhouette': float(metrics[ds_name]['DBSCAN']['silhouette']),\n",
    "            'davies_bouldin': float(metrics[ds_name]['DBSCAN']['davies_bouldin']),\n",
    "            'calinski_harabasz': float(metrics[ds_name]['DBSCAN']['calinski_harabasz'])\n",
    "        }\n",
    "    }\n",
    "\n",
    "with open('artifacts/metrics_summary.json', 'w') as f:\n",
    "    json.dump(metrics_summary, f, indent=2)\n",
    "\n",
    "print('сохранено: metrics_summary.json')\n",
    "\n",
    "best_configs = {\n",
    "    'ds1': {\n",
    "        'algorithm': 'KMeans',\n",
    "        'parameters': {'n_clusters': kmeans_results['ds1']['best_k']},\n",
    "        'reason': f\"Best silhouette score ({metrics['ds1']['KMeans']['silhouette']:.4f})\"\n",
    "    },\n",
    "    'ds2': {\n",
    "        'algorithm': 'DBSCAN',\n",
    "        'parameters': dbscan_results['ds2']['best_params'],\n",
    "        'reason': f\"Better noise handling ({metrics['ds2']['DBSCAN']['noise_ratio']*100:.1f}% noise)\"\n",
    "    },\n",
    "    'ds3': {\n",
    "        'algorithm': 'KMeans',\n",
    "        'parameters': {'n_clusters': kmeans_results['ds3']['best_k']},\n",
    "        'reason': f\"Stable with high noise ({metrics['ds3']['DBSCAN']['noise_ratio']*100:.1f}%)\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('artifacts/best_configs.json', 'w') as f:\n",
    "    json.dump(best_configs, f, indent=2)\n",
    "\n",
    "print('сохранено: best_configs.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6879a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('artifacts/labels', exist_ok=True)\n",
    "\n",
    "for i, ds_name in enumerate(['ds1', 'ds2', 'ds3'], 1):\n",
    "    X = preprocessed[ds_name]['X']\n",
    "    sample_id = preprocessed[ds_name]['sample_id']\n",
    "    \n",
    "    if i == 2:\n",
    "        labels = dbscan_results[ds_name]['best_model'].fit_predict(X)\n",
    "    else:\n",
    "        labels = kmeans_results[ds_name]['best_model'].fit_predict(X)\n",
    "    \n",
    "    df_labels = pd.DataFrame({\n",
    "        'sample_id': sample_id.values if hasattr(sample_id, 'values') else sample_id,\n",
    "        'cluster_label': labels\n",
    "    })\n",
    "    \n",
    "    df_labels.to_csv(f'artifacts/labels/labels_hw07_ds{i}.csv', index=False)\n",
    "    print(f'сохранено: artifacts/labels/labels_hw07_ds{i}.csv')\n",
    "\n",
    "print('\\nВсе артефакты сохранены.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hw07 (3.14.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
