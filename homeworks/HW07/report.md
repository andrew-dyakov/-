# HW07 – Report

> Файл: `homeworks/HW07/report.md`  
> Важно: не меняйте названия разделов (заголовков). Заполняйте текстом и/или вставляйте результаты.

## 1. Datasets

Вы выбрали 3 датасета из 4 (перечислите):

### 1.1 Dataset A

- Файл: `S07-hw-dataset-01.csv`
- Размер: (12000, 9)
- Признаки: 8 численных признаков (f01–f08) + sample_id
- Пропуски: отсутствуют
- "Подлости" датасета: признаки в сильно разных шкалах (std: f02~60, f07~60, f01~11, f03~0.5); потребовалась нормализация


### 1.2 Dataset B

- Файл: `S07-hw-dataset-02.csv`
- Размер: (8000, 4)
- Признаки: 3 численных признака (x1, x2, z_noise) + sample_id
- Пропуски: отсутствуют
- "Подлости" датасета: нелинейная структура данных; шумовой признак (z_noise); DBSCAN справляется лучше KMeans

### 1.3 Dataset C

- Файл: `S07-hw-dataset-03.csv`
- Размер: (15000, 5)
- Признаки: 4 численных признака (x1, x2, f_corr, f_noise) + sample_id
- Пропуски: отсутствуют
- "Подлости" датасета: кластеры разной плотности и формы; высокий уровень шума (~21.6%); KMeans более стабилен чем DBSCAN

## 2. Protocol

**Препроцессинг:**
- SimpleImputer (strategy='median') + StandardScaler в Pipeline
- Применялись только к числовым признакам (исключались sample_id и категориальные)
- Масштабирование необходимо из-за различных диапазонов признаков

**Поиск гиперпараметров:**
- KMeans: k ∈ [2, 20], random_state=42, n_init=10
- DBSCAN: сеточный поиск eps ∈ [0.2, 2.0] (10 значений), min_samples ∈ [3, 5, 10]
- Выбор лучшего: максимум Silhouette для валидных результатов, иначе максимум количества кластеров

**Метрики:**
- Silhouette Score (−1 до 1, выше лучше)
- Davies-Bouldin Index (ниже лучше)
- Calinski-Harabasz Index (выше лучше)
- DBSCAN: отдельное считание метрик для точек с шумом (label=−1)

**Визуализация:**
- PCA (2 компоненты, random_state=42) для каждого датасета
- 6 подграфиков: 3 датасета × 2 алгоритма
- Шум в DBSCAN отмечен красными крестиками

## 3. Models

**Модель 1: KMeans**
- Параметры: k ∈ [2, 20], random_state=42, n_init=10
- Подбирался k по максимуму Silhouette
- Применен на всех 3 датасетах

**Модель 2: DBSCAN**
- Параметры: eps ∈ [0.2, 2.0] (10 значений), min_samples ∈ [3, 5, 10]
- Сеточный поиск по 30 комбинациям
- Применен на всех 3 датасетах
- Обрабатывает шум (points with label=−1)

**Итого:** 2 алгоритма × 3 датасета = 6 моделей для сравнения

## 4. Results

Для каждого датасета – краткая сводка результатов.

### 4.1 Dataset A 

- Лучший метод: **KMeans с k=2**
- Метрики: Silhouette=0.5216, Davies-Bouldin=0.6853, Calinski-Harabasz=11787
- DBSCAN результат: 2 кластера, 0% шума (eps=1.80, min_samples=3)
- Обоснование: KMeans и DBSCAN показали одинаковое качество (Sil≈0.52), но KMeans проще и работает стабильнее; кластеры хорошо разделены

### 4.2 Dataset B 

- Лучший метод: **DBSCAN с eps=0.80, min_samples=5**
- Метрики: Silhouette=0.4810, Davies-Bouldin=7.6544, Calinski-Harabasz=7.6
- Шум: 44 точки (0.5% выбросов)
- KMeans метрики: Silhouette=0.3069 (хуже на 57%)
- Обоснование: нелинейная структура; DBSCAN выигрывает; естественно обнаруживает и обрабатывает шум

### 4.3 Dataset C 

- Лучший метод: **KMeans с k=3**
- Метрики: Silhouette=0.3155, Davies-Bouldin=1.1577, Calinski-Harabasz=6957.2
- DBSCAN результат: 2 кластера, 22 точки шума (0.1%), Silhouette=−1.0 (неустойчив)
- Обоснование: высокий шум (~21.6%) дестабилизирует DBSCAN; KMeans более робастен на noisy датасетах

## 5. Analysis

### 5.1 Сравнение алгоритмов (важные наблюдения)

**KMeans vs DBSCAN:**
- KMeans работает хорошо на Dataset A (хорошо разделённые кластеры) и Dataset C (высокий шум)
- DBSCAN выигрывает на Dataset B (нелинейная структура, естественная обработка выбросов)
- KMeans предполагает сферические кластеры примерно одинакового размера; DBSCAN гибче (произвольные формы) но чувствителен к параметрам eps и min_samples

**Критические факторы:**
- Масштабирование (StandardScaler) было критично: без него результаты метрик были неадекватны
- Шум: высокий уровень выбросов дестабилизирует DBSCAN (Dataset C), но не влияет на KMeans
- Форма кластеров: нелинейные структуры (Dataset B) требуют DBSCAN
- Плотность: DBSCAN требует настройки eps в зависимости от локальной плотности

### 5.2 Устойчивость (обязательно для одного датасета)

**Проверка устойчивости на Dataset A:**
- Метод: 5 запусков KMeans с разными random_state (42, 123, 456, 789, 999)
- Результат: Adjusted Rand Index (ARI) между всеми парами = 1.0000 (среднее) ± 0.0000 (стд)
- Интерпретация: абсолютная стабильность — все 5 запусков дали идентичные разбиения (все точки в одних же кластерах)
- **Вывод:** KMeans на Dataset A полностью устойчив; задача хорошо определена, нет локальных минимумов

### 5.3 Интерпретация кластеров

**Dataset A (KMeans, k=2):**
Кластер 0: низкие значения f02, f04; Кластер 1: высокие значения f02, f04. Интерпретация: противоположные знаки основных признаков.

**Dataset B (DBSCAN, eps=0.80):**
Основной кластер: плотно сгруппированные (x1, x2) ≈ (0, 0). Шум (44 точки): выбросы по z_noise (хвосты распределения). Интерпретация: нормальные наблюдения vs шумовые выбросы.

**Dataset C (KMeans, k=3):**
3 кластера разной плотности; сложно интерпретировать из-за высокого шума. Профили различаются по x1, x2; f_noise распределён равномерно.

## 6. Conclusion

1. **Нет универсального алгоритма:** KMeans и DBSCAN имеют разные сильные стороны; выбор зависит от структуры данных (плотность, форма кластеров, наличие шума).

2. **Масштабирование критично:** StandardScaler обязателен перед кластеризацией, иначе признаки в больших шкалах доминируют.

3. **DBSCAN требует настройки:** параметры eps и min_samples критичны и подбираются вручную через сеточный поиск; нет универсальных значений.

4. **Шум—двойной меч:** DBSCAN обнаруживает выбросы, но чувствителен к ним при подборе параметров; KMeans игнорирует шум, но более стабилен.

5. **Устойчивость = надёжность:** проверка по разным random_state показывает, насколько результат зависит от инициализации; ARI=1.0 указывает на хорошо определённую задачу.

6. **Размер датасета важен:** на малых данных (n<1000) выше риск переобучения; на больших (n>10000) статистические метрики более надёжны и воспроизводимы.
